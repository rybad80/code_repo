{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "def nzsql_to_mssql_ddl(table_name,src_system):\n",
    "#function to compile CREATE TABLE statement and convert mssql to nzsql data types    \n",
    "    if   src_system == 'SENSIS':\n",
    "        conn_str = pyodbc.connect(r'DSN=Sensis-CATSENPW1;UID=local_reader;PWD=$enS1s_R3ad')\n",
    "    elif src_system == 'JOCAPS':\n",
    "        conn_str = 'Driver={NetezzaSQL};servername=CHOPDW;port=5480;database=CDWPRD;username=rybad;password=qpwoeiruty1029384756YYY;'        \n",
    "    elif src_system == 'SYNGOECHO':\n",
    "        conn_str = pyodbc.connect(r'Syngo Echo - AcusonDB;CCIS_Data_Access=local_reader;PWD=D@t@Acc3ss')\n",
    "    elif src_system == 'MUSE':\n",
    "        conn_str = pyodbc.connect(r'DSN=Muse;UID=CCIS_Data_Access;PWD=D@t@Acc3ss') \n",
    "    elif src_system == 'CENTRIPETUS':\n",
    "        conn_str = pyodbc.connect(r'DSN=Centripetus;UID=CARDIO;PWD=access')\n",
    "    elif src_system == 'CENTRIPETUS_DEV':\n",
    "        conn_str = pyodbc.connect(r'DSN=CENTRIPETUS-DEV;UID=CARDIO;PWD=access')            \n",
    "    elif src_system == 'CENTRIPETUS-Test':\n",
    "        conn_str = pyodbc.connect(r'DSN=Centripetus-Test;UID=CARDIO;PWD=access')\n",
    "    elif src_system == 'CLINIBASE':\n",
    "        conn_str = pyodbc.connect('DRIVER={SQL Server};Server=QSQLA177;DATABASE=Clinibase_Repository;UID=CCIS_Data_Access;PWD=Cc1$0a!@') \n",
    "    elif src_system == 'HEALTHVIEW':\n",
    "        conn_str = pyodbc.connect('DRIVER={SQL Server};Server=PSQLA129;DATABASE=Apollo;UID=CCIS_Data_Access;PWD=D@t@Acc3ss') \n",
    "    \n",
    "    query = '''\n",
    "               SELECT\n",
    "                   COLUMN_NAME,\n",
    "                   DATATYPE,\n",
    "                   CASE WHEN COLUMN_ORDER = 1 THEN NULLABLE ELSE NULLABLE+',' END AS NULLABLE\n",
    "                FROM\n",
    "                    (  \n",
    "                    SELECT  \t\n",
    "                                UPPER(c.name) COLUMN_NAME,\n",
    "                                CASE WHEN typ.name in ('datetime' ,'smalldatetime')\n",
    "                                      THEN 'VARCHAR(50)'\n",
    "                                      WHEN typ.name ='BIT'\n",
    "                                      THEN 'BOOLEAN'\n",
    "                                      WHEN typ.name ='MONEY'\n",
    "                                      THEN 'VARCHAR(10)'\n",
    "                                      WHEN typ.name ='TINYINT'\n",
    "                                      THEN 'VARCHAR(1)'                                           \n",
    "                                     WHEN typ.name = 'varchar' \n",
    "                                      THEN UPPER(typ.name)+'('+cast(abs(c.max_length) as varchar(255))+')'\n",
    "                                     WHEN typ.name in ('nchar','nvarchar') \n",
    "                                      THEN UPPER(typ.name)+'('+cast(abs(c.max_length/2) as varchar(255))+')'\n",
    "                                   ELSE UPPER(typ.name) END AS DATATYPE,\n",
    "                                CASE WHEN c.is_nullable = 0 THEN 'NOT NULL' ELSE 'NULL' END AS NULLABLE,\n",
    "                                C.column_id,\n",
    "                                ROW_NUMBER() OVER (PARTITION BY TBL.name ORDER BY C.COLUMN_ID DESC) COLUMN_ORDER\n",
    "\n",
    "                    FROM    \n",
    "                        sys.columns c\tINNER JOIN\tSYS.tables tbl ON C.object_id = TBL.object_id\n",
    "                                        INNER JOIN\tsys.types typ ON c.user_type_id = typ.user_type_id\n",
    "                                            LEFT JOIN (select ic.object_id,ic.index_id,ic.column_id, i.is_primary_key \n",
    "                                                        from sys.index_columns ic \n",
    "                                                             inner JOIN sys.indexes i \n",
    "                                                               ON ic.object_id = i.object_id AND ic.index_id = i.index_id \n",
    "                                                               AND is_primary_key = 1) ic\n",
    "                                                  ON ic.object_id = c.object_id AND ic.column_id = c.column_id\t\t\t\t\t     \n",
    "                    WHERE tBL.name = '{table_name}'\n",
    "                     ) A\n",
    "               ORDER BY column_id\n",
    "                 ''' \n",
    "    ddl_query = query.format(table_name=table_name)  \n",
    "\n",
    "    ddl=pd.read_sql(ddl_query,conn_str)\n",
    "\n",
    "    return ddl.to_string(index=False,header=None);\n",
    "\n",
    "\n",
    "\n",
    "def ddl_pk(table_name):\n",
    "#function to identify primary key(s) on the table  \n",
    "    if src_system == 'SENSIS':\n",
    "        conn_str = pyodbc.connect(r'DSN=Sensis-CATSENPW1;UID=local_reader;PWD=$enS1s_R3ad')\n",
    "    elif src_system == 'SYNGOECHO':\n",
    "        conn_str = pyodbc.connect(r'Syngo Echo - AcusonDB;CCIS_Data_Access=local_reader;PWD=D@t@Acc3ss')\n",
    "    elif src_system == 'MUSE':\n",
    "        conn_str = pyodbc.connect(r'DSN=Muse;UID=CCIS_Data_Access;PWD=D@t@Acc3ss') \n",
    "    elif src_system == 'CENTRIPETUS':\n",
    "        conn_str = pyodbc.connect(r'DSN=Centripetus;UID=CARDIO;PWD=access')\n",
    "    elif src_system == 'CENTRIPETUS_DEV':\n",
    "        conn_str = pyodbc.connect(r'DSN=CENTRIPETUS-DEV;UID=CARDIO;PWD=access')            \n",
    "    elif src_system == 'CLINIBASE':\n",
    "        conn_str = pyodbc.connect('DRIVER={SQL Server};Server=QSQLA177;DATABASE=Clinibase_Repository;UID=CCIS_Data_Access;PWD=Cc1$0a!@')        \n",
    "    elif src_system == 'HEALTHVIEW':\n",
    "        conn_str = pyodbc.connect('DRIVER={SQL Server};Server=PSQLA129;DATABASE=Apollo;UID=CCIS_Data_Access;PWD=D@t@Acc3ss')         \n",
    "           \n",
    "    query = '''\n",
    "                SELECT distinct\n",
    "                            STUFF((\n",
    "                            SELECT ',' + UPPER(c1.name )\n",
    "                            FROM sys.columns c1\t\n",
    "                            WHERE c1.OBJECT_ID=C2.OBJECT_ID  and is_nullable = 0\n",
    "                            FOR XML PATH ('')\n",
    "                        ), 1, 1, '')\n",
    "                FROM sys.columns c2\tINNER JOIN\tSYS.tables tbl2 ON C2.object_id = TBL2.object_id\n",
    "                WHERE tBL2.name = '{table_name}'  and c2.is_nullable = 0\n",
    "                 '''\n",
    "    pk_query = query.format(table_name=table_name)  \n",
    "\n",
    "    pk=pd.read_sql(pk_query,conn_str)\n",
    "    \n",
    "    if pk.empty == False:\n",
    "        return pk.to_string(index=False,header=None); \n",
    "\n",
    "    else: \n",
    "        return '';    \n",
    "\n",
    "def run_ddl(sql, conn_str):\n",
    "#function to execute DDL\n",
    "\n",
    "    if tgt_system == 'CDW_ODS_DEV':\n",
    "        conn_str = pyodbc.connect('Driver={NetezzaSQL};servername=uat.cdw.chop.edu;port=5480;database=CDW_ODS_DEV;username=rybad;password=qpwoeiruty1029384756YYY')\n",
    "    \n",
    "    cursor = conn_str.cursor()\n",
    "    cursor.execute(sql)\n",
    "    conn_str.commit()\n",
    "    \n",
    "def get_cnxn(env='dev'):\n",
    "    '''returns a pyodbc connection to cdw dev or prod'''\n",
    "    if env == 'JOCAPS':\n",
    "        cnxn_str = 'Driver={NetezzaSQL};servername=CHOPDW;port=5480;database=CDWPRD;username=rybad;password=qpwoeiruty1029384756YYY;'\n",
    "    elif env == 'SENSIS':\n",
    "        cnxn_str = 'DSN=Sensis-CATSENPW1;UID=local_reader;PWD=$enS1s_R3ad;'        \n",
    "    elif env == 'uat':\n",
    "        cnxn_str = 'Driver={NetezzaSQL};servername=uat.cdw.chop.edu;port=5480;database=CDW_ODS_UAT;username=;password=qpwoeiruty1029384756YYY;'\n",
    "    elif env == 'CLINIBASE':\n",
    "        cnxn_str = 'DRIVER={SQL Server};Server=QSQLA177;DATABASE=Clinibase_Repository;UID=CCIS_Data_Access;PWD=Cc1$0a!@;'                \n",
    "    elif env == 'MUSE':\n",
    "        cnxn_str = 'DSN=Muse;UID=CCIS_Data_Access;PWD=D@t@Acc3ss'   \n",
    "    elif env == 'CENTRIPETUS':\n",
    "        cnxn_str = 'DSN=Centripetus;UID=CARDIO;PWD=access'\n",
    "    elif env == 'CENTRIPETUS_DEV':\n",
    "        cnxn_str = 'DSN=CENTRIPETUS-DEV;UID=CARDIO;PWD=access'      \n",
    "    elif env == 'HEALTHVIEW':\n",
    "        cnxn_str = 'DSN=HEALTHVIEW;UID=CCIS_Data_Access;PWD=D@t@Acc3ss' \n",
    "    else:\n",
    "        cnxn_str = 'Driver={NetezzaSQL};servername=uat.cdw.chop.edu;port=5480;database=CDW_ODS_DEV;username=rybad;password=qpwoeiruty1029384756YYY;'\n",
    "    return pyodbc.connect(cnxn_str)    \n",
    "\n",
    "\n",
    "\n",
    "def execute_sql(sql, env='dev'):\n",
    "    cnxn = get_cnxn(env)\n",
    "    cursor = cnxn.cursor()\n",
    "    row_ct = cursor.execute(sql).rowcount\n",
    "    cnxn.commit()\n",
    "    return row_ct\n",
    "\n",
    "def sql_to_df(sql, env='dev', quiet=False):\n",
    "    '''executes the sql provided against cdw dev or prod'''\n",
    "    cnxn = get_cnxn(env)\n",
    "    df = pd.read_sql(sql, cnxn)\n",
    "    return df\n",
    "\n",
    "def df_clean_strings(df):\n",
    "    '''strips all strings in df and removes all chars in bad_chars list'''\n",
    "    df = df.applymap(lambda x: x.strip() if type(x) is str else x)\n",
    "    for ch in BAD_CHARS:\n",
    "        df = df.applymap(lambda x: x.replace(ch, '') if type(x) is str else x)\n",
    "    return df\n",
    "\n",
    "def df_round_floats(df):\n",
    "    '''rounding float characters upto 5 decimals'''\n",
    "    df = df.applymap(lambda x: round(x,FLOAT_DIGITS) if type(x) is float else x)\n",
    "    return df\n",
    "\n",
    "def infer_dtypes(df, as_is_cols=[]):\n",
    "    \"\"\"\n",
    "    Infer datatypes of a pandas dataframe and their Netezza equivalent.\n",
    "    Will not change the type of any column in the exclude list.\n",
    "    Returns a dict of col_name -> NETEZZA data types.\n",
    "    \"\"\"\n",
    "    data_types = {}\n",
    "    netezza_types = {\n",
    "        'int64': 'BIGINT',\n",
    "        'float64': 'DECIMAL(18,' + str(FLOAT_DIGITS) + ')',\n",
    "        'datetime64[ns]': 'DATETIME',#DATE\n",
    "        'object': 'VARCHAR',\n",
    "        'bool': 'BOOLEAN'}\n",
    "    for col_name in df.columns:\n",
    "        my_dtype = df[col_name].dtype\n",
    "        if df[col_name].dtype == 'bool':\n",
    "            data_types[col_name] = netezza_types['bool']\n",
    "        elif df[col_name].dtype == 'object':\n",
    "            data_types[col_name] = netezza_types['object']      \n",
    "        elif df[col_name].dtype == 'int64':\n",
    "            data_types[col_name] = netezza_types['int64']\n",
    "        elif df[col_name].dtype == 'float64':\n",
    "            data_types[col_name] = netezza_types['float64']\n",
    "        elif df[col_name].dtype == 'datetime64[ns]':\n",
    "            data_types[col_name] = 'VARCHAR'         \n",
    "        else:\n",
    "            data_types[col_name] = netezza_types['varchar']\n",
    "    return data_types\n",
    "\n",
    "def infer_col_lengths(df, data_types):\n",
    "    \"\"\"\n",
    "    Gets the longest value in each columns for strings.\n",
    "    Needed to specify varchar lengths.\n",
    "    \"\"\"\n",
    "    col_lengths = {}\n",
    "    for col_name in df.columns:\n",
    "        if data_types[col_name] == 'VARCHAR':\n",
    "            # find the longest string in the column\n",
    "            if math.isnan(df[col_name].astype(str).str.len().max()):\n",
    "                col_lengths[col_name]='(50)'\n",
    "            else:\n",
    "                max_rec_len = max(1,int(df[col_name].astype(str).str.len().max()))\n",
    "                #print(str(max_rec_len))\n",
    "                #print(df[col_name])\n",
    "                col_lengths[col_name] = '(' + str(max_rec_len+16) + ')'        \n",
    "                #col_lengths[col_name] = '(100)'     \n",
    "        else:\n",
    "            # we dont need to do any length specs for ints, floats, datetimes\n",
    "            col_lengths[col_name] = ''\n",
    "    return col_lengths\n",
    "\n",
    "def infer_col_names(df):\n",
    "    \"\"\"\n",
    "    Just need to uppercase-ize and remove and blanks from column names.\n",
    "    We also have some checks for bad columns names, will append _\n",
    "    \"\"\"\n",
    "    headers = {}\n",
    "    #key_words = ['USER', 'HOURS', 'TIME', 'POSITION', 'COMMENT', 'ORDER', 'SHOW']\n",
    "    for col_name in df.columns:\n",
    "        header = col_name.upper()\n",
    "        header = header.replace(' ', '_')\n",
    "        header = header.replace('.', '_')\n",
    "        header = header.replace('/', '_per_')\n",
    "        #if header in key_words:\n",
    "            #header = header + '_'\n",
    "        headers[col_name] = header\n",
    "    return headers\n",
    "\n",
    "def drop_table_sql(table_name):\n",
    "    '''\n",
    "    returns the sql needed to drop a table if it exists in netezza\n",
    "    '''\n",
    "    return 'drop table ' + table_name + ' if exists;'\n",
    "\n",
    "def nzload_sql(table_name, load_file, log_dir, header_dict, dtype_dict,\n",
    "               longest_dict):\n",
    "    '''\n",
    "    creates the nzload sql for loading to cdw\n",
    "    '''\n",
    "    nz_sql = \"create table \" + table_name + \" as \\n\"            + \"select * from external '\" + load_file + \"' (\"\n",
    "    for col_name in header_dict:\n",
    "        nz_sql = nz_sql + \"\\n \" + str(header_dict[col_name]) + \" \"                + str(dtype_dict[col_name]) + str(longest_dict[col_name]) + \",\"\n",
    "    # remove last character to handle dangling sql comma\n",
    "    nz_sql = nz_sql[:-1] + \")\" + '''\n",
    "    USING (\n",
    "      REMOTESOURCE 'ODBC'\n",
    "      DELIMITER ','\n",
    "      SKIPROWS 1\n",
    "      MAXERRORS 1\n",
    "      ENCODING 'internal'\n",
    "      LOGDIR '__logdir__'\n",
    "      DATESTYLE YMD\n",
    "      DATEDELIM '-'\n",
    "      TIMESTYLE '24HOUR'\n",
    "      TIMEDELIM ':'\n",
    "      QUOTEDVALUE Double\n",
    "      BoolStyle TRUE_FALSE\n",
    "    );\n",
    "'''\n",
    "    nz_sql = nz_sql.replace('__logdir__', log_dir)\n",
    "    #print(nz_sql)\n",
    "    return nz_sql\n",
    "\n",
    "def df_to_cdw(df, table_name, prod_or_dev='dev', grant_access=True,\n",
    "              as_is_cols=[], hist_sched='ad hoc', hist_type='unknown',\n",
    "              hist_source_id=-999, quiet=False):\n",
    "    '''creates a table in CDW via NZ Load with the contents of df\n",
    "    This performs a full drop and reload, and thus will replace any data that\n",
    "    already exists. It creates a csv in the current working directory for\n",
    "    loading. The load log is also created:\n",
    "    ../cwd/load_files/__table_name__.csv\n",
    "    ../cwd/load_logs/\n",
    "    This function performs some simple cleaning a dtype inference.\n",
    "    grant_access: by default, access will be granted to all users of the \n",
    "    new table.\n",
    "    If your date columns are not already formatted as dates, they will be\n",
    "    created as VARCHAR in CDW. Use something like this to format them first:\n",
    "    df[col] = pd.to_datetime(df[col])\n",
    "    '''\n",
    "    starting_rows = len(df)\n",
    "    if starting_rows == 0:\n",
    "        print('Warning: No records in DF, no table created')\n",
    "        return df\n",
    "    df = df_clean_strings(df)\n",
    "    df = df_round_floats(df)\n",
    "    data_types = infer_dtypes(df, as_is_cols=as_is_cols)\n",
    "    col_lengths = infer_col_lengths(df, data_types)\n",
    "    headers = infer_col_names(df)\n",
    "    if not os.path.exists(LOAD_FILE_PATH):\n",
    "        os.makedirs(LOAD_FILE_PATH)\n",
    "    load_file = os.path.join(LOAD_FILE_PATH, table_name + '.csv')\n",
    "    df.to_csv(os.path.join(load_file), index=False)\n",
    "    if not os.path.exists(LOG_FILE_PATH):\n",
    "        os.makedirs(LOG_FILE_PATH)\n",
    "    drop_sql = drop_table_sql(table_name)\n",
    "    execute_sql(drop_sql, prod_or_dev)\n",
    "    nz_sql = nzload_sql(table_name, load_file, LOG_FILE_PATH,\n",
    "                        headers, data_types, col_lengths)\n",
    "    loaded_rows = execute_sql(nz_sql, prod_or_dev) #this step executes the entire load\n",
    "    #print(nz_sql) #uncomment this to obtain the load script for manual runs\n",
    "    return 'records successfully loaded'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# SET VARIABLES HERE\n",
    "\n",
    "table_list = (\n",
    "#'FETAL_IMAGES',\n",
    "#'ADDITIONALSTUDIES_IMAGES',\n",
    "#'EVENT_FETAL',\n",
    "#'EP_IMAGES',\n",
    "#'EVENT_ADDITIONALSTUDIES',\n",
    "#'HISTORY_IMAGES',\n",
    "#'EVENT_HISTORY',\n",
    "#'CARDIACCTMR_IMAGES',\n",
    "#'CHOP_STS_IMAGES',\n",
    "#'EVENT_CARDIACCTMR',\n",
    "#'STS_IMAGES',\n",
    "#'STRESS_IMAGES',\n",
    "#'EVENT_STRESS',\n",
    "#'CATH_IMAGES',\n",
    "#'ADMIT_IMAGES',\n",
    "#'HOLTER_IMAGES',\n",
    "#'PED_ECHO_IMAGES',\n",
    "#'ARU_IMAGES',\n",
    "'EVENT_PEDECHO', #--__PREFIX column failure\n",
    "#'EVENT_ARU',\n",
    "#'ECG_IMAGES',\n",
    "#'EVENT_ECG',\n",
    "#'EVENT_LABORATORY',\n",
    "#'EVENT_MEDICATIONS',\n",
    "#'EVENT_ADMISSION',\n",
    "#'EVENT_CATH',\n",
    "#'EVENT_EP',\n",
    "#'EVENT_CORRESPONDENCE',\n",
    "#'EVENT_STS',\n",
    "#'EVENT_HOLTER',\n",
    "'DEMOGRAPHICS',\n",
    "\n",
    ")\n",
    "\n",
    "#, CARDIAC_OUTPUTS','HEMODYNAMIC','OXIMETRY','PD_CALCULATION','PDRESULT','RESISTANCE','PDSHUNTVALUE','PDCALCSHUNT','VALVE','PDCALCRESIST','PDRESISTVALUE')\n",
    "#table_list = ('DICSITE',)\n",
    "read_table_list = table_list\n",
    "#split_table_list = read_table_list.split(',')\n",
    "\n",
    "\n",
    "#src_system = 'SENSIS'\n",
    "src_system = 'HEALTHVIEW'\n",
    "table_prefix = src_system\n",
    "tgt_system = 'CDW_ODS_DEV'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDL File Creation Complete\n",
      "Table Creation complete\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "#create ddl to file - only run if DDL needed\n",
    "#######################################\n",
    "\n",
    "ddl_file_name = os.getcwd()+\"\\\\\"+f'{src_system}_DATALAKE_DDL.sql'\n",
    "#print(os.getcwd())\n",
    "ddl_file = open(ddl_file_name,\"w+\")\n",
    "# loop through all table names and compile DDL    \n",
    "for val in read_table_list:\n",
    "    table_name = val\n",
    "    ddl = nzsql_to_mssql_ddl(table_name,src_system)\n",
    "    pk = ddl_pk(table_name)\n",
    "    if pk == '':\n",
    "        ddl_print =     (f''' \n",
    "                       DROP TABLE {table_prefix}_{table_name} IF EXISTS;\n",
    "                      CREATE TABLE {table_prefix}_{table_name} \n",
    "                      (\n",
    "                      {ddl}\n",
    "                       )\n",
    "                      DISTRIBUTE ON RANDOM;\n",
    "                      ''')\n",
    "    else:\n",
    "        ddl_print =     (f''' \n",
    "                    DROP TABLE {table_prefix}_{table_name} IF EXISTS;\n",
    "                      CREATE TABLE {table_prefix}_{table_name} \n",
    "                     (\n",
    "                     {ddl})\n",
    "                      DISTRIBUTE ON RANDOM;\n",
    "                      ALTER TABLE {src_system}_{table_name} ADD CONSTRAINT PK_{src_system}_{table_name} PRIMARY KEY ({pk});\n",
    "                      ''')\n",
    "    #print(ddl_print)\n",
    "    ddl_file.write(ddl_print)\n",
    "\n",
    "\n",
    "ddl_file.close()\n",
    "#print(ddl_file)\n",
    "print('DDL File Creation Complete')\n",
    "\n",
    "#execute table creation and PKs from DDL file\n",
    "#ddl_read = open(ddl_file_name,\"r\")\n",
    "#ddl_string = ddl_read.read().replace('\\n','')\n",
    "#run_ddl(ddl_string,'CDW_ODS_DEV')\n",
    "\n",
    "print('Table Creation complete')\n",
    "\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVENT_PEDECHO load begin\n",
      "loading 676106 rows\n",
      "HEALTHVIEW_EVENT_PEDECHO load complete\n",
      "DEMOGRAPHICS load begin\n",
      "loading 1113878 rows\n",
      "HEALTHVIEW_DEMOGRAPHICS load complete\n"
     ]
    }
   ],
   "source": [
    "BAD_CHARS = ['\"', '\\n', '\\r']\n",
    "\n",
    "FLOAT_DIGITS = 5\n",
    "NZ_LOG_PATH='C:\\\\Users\\\\rybad\\\\Desktop\\\\'\n",
    "LOAD_FILE_PATH = os.path.join(NZ_LOG_PATH,'load_files')\n",
    "LOG_FILE_PATH = os.path.join(NZ_LOG_PATH,'log_files')\n",
    "\n",
    "# loop through all table names, select * from table and load to target\n",
    "for val in read_table_list:\n",
    "    src_table_name = val\n",
    "    select_star_sql =   (f''' \n",
    "                         SELECT * \n",
    "                         FROM {src_table_name} \n",
    "                         \n",
    "                         ''')\n",
    "   # select_star_sql = (\n",
    "   # f'''SELECT * \n",
    "   #         FROM\n",
    "   #         TABLE WITH FINAL (CDW_LOGICAL.ADMIN.FQREAD('biBIGSQL','', ' SELECT * FROM JOCAPS.{src_table_name} '))'''\n",
    "   # )\n",
    "   # print(src_system)\n",
    "    print(f'{src_table_name} load begin')\n",
    "    df=sql_to_df(select_star_sql,env=src_system)\n",
    "    len_df = len(df)\n",
    "    print(f'loading {len_df} rows')\n",
    "    tgt_table_name = f'{src_system}_{src_table_name}'\n",
    "    df_to_cdw(df=df, table_name=tgt_table_name, prod_or_dev='dev')\n",
    "    \n",
    "    print(f'{tgt_table_name} load complete')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
